{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Radio Lightcurves\n",
    "\n",
    "**Lecturer:** Dougal Dobie<br>\n",
    "**Jupyter Notebook Authors:** Dougal Dobie, David Kaplan\n",
    "\n",
    "## Key steps\n",
    "- Plot the radio light curve\n",
    "- Determine its spectral index\n",
    "- Scale the data based on its spectral index\n",
    "- Fit the data with a power law\n",
    "- Fit the data with a broken power law with a smooth turnover\n",
    "\n",
    "## Required dependencies\n",
    "\n",
    "* python 3\n",
    "* astropy\n",
    "* numpy\n",
    "* scipy\n",
    "* matplotlib\n",
    "* emcee\n",
    "* corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "The radio properties of relativistic afterglows give a wealth of information on the properties of the explosion.  Back in the GRB days [REF] this was used to great effect.  We are now trying to use the same techniques on multi-messenger transients like binary neutron star mergers.  In particular, we will determine the basic time and spectral behavior of the radio lightcurve, which can be fit to determine the explosion energy, the surrounding circum-merger density, and other properties.\n",
    "\n",
    "Here we will look at observations of GW170817 (published in Dobie et al. 2018 and Mooley et al. 2018, among others).  This consists of measurements of flux density $S_\\nu(\\nu,t)$ with several radio telescopes (Jansky Very Large Array, Australia Telescope Compact Array, and Giant Metrewave Radio Telescope) as a function of time since explosion (in days) and frequency (in GHz).  Flux density measurements are in units of $\\mu\\mathrm{Jy}$, where $1\\,\\mathrm{Jy}=10^{-26}\\,\\mathrm{erg/s/cm^2/Hz}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries: load in python modules.  The uses of these will become clear as we go forward\n",
    "import numpy as np\n",
    "from astropy.io import ascii\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from scipy.optimize import least_squares, curve_fit\n",
    "from scipy.stats import f\n",
    "import emcee\n",
    "import corner\n",
    "import os\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Load the data\n",
    "We write a basic function to load in the radio lightcurve.  It's pretty easy using `astropy`.  But you should look at the file `radio_lightcurve.dat` and make sure that the data returned look appropriate.  It's also good to keep track of some basic info like:\n",
    "1. How many observations are there?\n",
    "2. What is the first observation date?  What is the last?\n",
    "3. What is the lowest frequency?  What is the highest frequency?\n",
    "4. Do all of the observations actually report _detections_, where the source is detected at $>3\\sigma$ significance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename='radio_lightcurve.dat'):\n",
    "    data = ascii.read(filename)\n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the data in two ways: `print(data.columns)` will print out the names of each column in the data table, and you can then examine the contents of each column using `print(data['column_name'])`.\n",
    "\n",
    "You can also select subsets of the data using commands like `np.where(data['delta_t']>50` which will give the index of every row after 50 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TableColumns names=('delta_t','telescope','frequency','flux','rms')>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Make a plot of the radio lightcurve\n",
    "We will now plot the flux density as a function of time (the lightcurve). We use different coloured markers to denote the observing frequency, and different marker styles to denote different telescopes.\n",
    "\n",
    "If you feel confident with Python, feel free to change edit the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ax, sm, data, scaled=False, **kwargs):\n",
    "    telescope_marker_dict = {'VLA':'s', 'ATCA':'o', 'GMRT':'d'}\n",
    "    \n",
    "    \n",
    "    for row in data:\n",
    "        #Loop over each row of the data, set the marker colour based on frequency and the marker style based on telescope\n",
    "        freq = row['frequency']\n",
    "        colorval = sm.to_rgba(freq) #Set the marker colour based on observing frequency\n",
    "        \n",
    "        telescope = row['telescope']\n",
    "        marker = telescope_marker_dict[telescope] #Set the marker based on telescope\n",
    "        \n",
    "        #Are we plotting the original flux density values? Or the scaled values?\n",
    "        if scaled:\n",
    "            flux = row['scaled_flux']\n",
    "            rms = row['scaled_rms']\n",
    "        else:\n",
    "            flux = row['flux']\n",
    "            rms = row['rms']\n",
    "        \n",
    "        ax.errorbar(row['delta_t'], flux, rms, linestyle='', marker=marker, c=colorval, **kwargs) #Plot the flux density (with uncertainty) on the provided axis\n",
    "    return\n",
    "\n",
    "def cmap_setup(cmap='viridis', min_freq=0, max_freq=17):\n",
    "    '''\n",
    "    This function will set up a scalar map for you to colour your markers by frequency\n",
    "    '''\n",
    "    freq_cmap = plt.cm.get_cmap(cmap)\n",
    "    \n",
    "    cNorm  = colors.Normalize(vmin=min_freq, vmax=max_freq)\n",
    "    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "    sm = scalarMap\n",
    "    sm._A = []\n",
    "    \n",
    "    return sm    \n",
    "    \n",
    "def make_plot(data, scaled=False, model=None, params=None, tvals=np.arange(10,400), plot_models=False):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    #Get the scalar map, plot the data using the plot_data function above\n",
    "    sm = cmap_setup()\n",
    "    plot_data(ax, sm, data, scaled=scaled)\n",
    "    \n",
    "    \n",
    "    #Set up a colourbar\n",
    "    cbar = fig.colorbar(sm,fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Frequency (GHz)')\n",
    "    \n",
    "    #Set axis scales to log\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    #Label axes, set axis limits etc.\n",
    "    ax.set_xlabel('Time (days)')\n",
    "    if scaled:\n",
    "        ax.set_ylabel('Scaled Flux Density ($\\mu$Jy)')\n",
    "    else:\n",
    "        ax.set_ylabel('Flux Density ($\\mu$Jy)')\n",
    "        \n",
    "    if model:\n",
    "        plot_model(model, params, tvals, ax)\n",
    "    \n",
    "    if plot_models:\n",
    "        plot_physical_models(ax)\n",
    "    \n",
    "    ax.set_xlim(10,300)\n",
    "    \n",
    "make_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Determining the spectral index\n",
    "As with many things in astrophysics, the emission in the radio regime is _non-thermal_ in origin (unlike the early emission in the optical/UV/infrared, which is a thermal blackbody).  What that means is that generally has a power-law spectrum:\n",
    "$$ S_\\nu(\\nu) \\propto \\nu^\\alpha$$\n",
    "Strictly this only works for data observed at _exactly_ the same time, but real observations don't work that way.  A single telescope can (usually) only observe at a single frequency, and different telescopes are separated in time by schedules, longitude, etc.  So we need to be a bit more generous about how we define simultaneous. Luckily, the light-curve is mostly evolving pretty slowly, so this isn't necessarily a problem.\n",
    "\n",
    "Here are two functions to take a subset of the data that was observed roughly simultaneously and calculate the spectral index $\\alpha$ and its uncertainty using multi-band observation at 162 days post-merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_power_law(freq,S0,alpha):\n",
    "    S = S0 * (freq) ** alpha\n",
    "    return S\n",
    "\n",
    "def alpha_calc(data):    \n",
    "    #Get lightcurve values\n",
    "    freqs = data['frequency']\n",
    "    flux = data['flux']\n",
    "    flux_errs = data['rms']\n",
    "    \n",
    "    #Use the scipy curve_fit algorithm to calculate the best fit value\n",
    "    popt, pcov = curve_fit(calc_power_law, freqs, flux ,sigma=flux_errs, p0=(50,-0.61),absolute_sigma=True)\n",
    "    \n",
    "    alpha = popt[1] #Best-fit spectral index\n",
    "    alpha_err = np.sqrt(np.diag(pcov))[1] #Uncertainty in alpha\n",
    "    \n",
    "    return alpha, alpha_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the data at the ~162 day epoch and print the spectral index + uncertainty\n",
    "\n",
    "sel_data = data[data['delta_t'] == 162.89]\n",
    "\n",
    "alpha, alpha_err = alpha_calc(sel_data)\n",
    "\n",
    "print(\"alpha = %.1f+/-%.1f\"%(alpha, alpha_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. What do you get for $\\alpha$ and its uncertainty?\n",
    "2. Do you think using a finite time window introduced significant errors into this calculation?\n",
    "3. What happens if you use a larger time range?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scaling the data based on the spectral Index\n",
    "Based on the $\\alpha$ you calculated above, what happens if you assume that _all_ of the observations can be fit using the same frequency power-law?  i.e., if $\\alpha$ were the same at all times?  If this is the case then scaling all of the data to a single frequency makes it easier to understand the temporal properties of the lightcurve as a function of only 1 variable, not 2.\n",
    "\n",
    "Write a function to take the observed data and scale it to a specific frequency based on an estimated spectral index. \n",
    "\n",
    "Don't forget to include uncertainties! You should add two columns to your data table called \"scaled_flux\" and \"scaled_rms\".\n",
    "\n",
    "Questions:\n",
    "1. Can you think of reasons why the spectral index should stay the same?  Why it should change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, alpha, alpha_err, ref_freq=3.0):\n",
    "    #calculate a scaling factor for the flux density and uncertainty\n",
    "    f_scale = ######\n",
    "    rms_scale = ######\n",
    "    \n",
    "    #scale the flux and uncertainty - don't forget to add errors in quadrature\n",
    "    scaled_flux = ######\n",
    "    scaled_rms = ######\n",
    "    \n",
    "    #Add two new columns to the data\n",
    "    data['scaled_flux'] = scaled_flux\n",
    "    data['scaled_rms'] = scaled_rms\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data to 3 GHz based on your estimated spectral index and associated uncertainty, then plot the scaled lightcurve by passing `scaled=True` to your `make_plot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scale_data(data, alpha, alpha_err)\n",
    "make_plot(data, scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Fitting the data\n",
    "We now want to characterise the radio lightcurve. You should be able to see that it initially rises according to a power law, peaks somewhere between 100 and 200 days post-merger and then declines according to a different power law.\n",
    "\n",
    "Create a new table called tdata that we will use to determine the properties of the lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "tdata = scale_data(data, -0.6,0.05)\n",
    "\n",
    "make_plot(tdata, scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this data with a \"smoothed broken power law\", which combines two power laws with a smoothing parameter around the break point. One functional form of this is given by\n",
    "\n",
    "$S_\\nu(t) = S_{\\rm \\nu,peak} \\left[ \\left(\\dfrac{t}{t_{\\rm peak}}\\right)^{-s\\delta_1} + \\left(\\dfrac{t}{t_{\\rm peak}}\\right)^{-s\\delta_2}\\right]^{-1/s}$\n",
    "\n",
    "Here the spectral index is still $\\alpha$, but we've also introduced _temporal_ power-law indices $\\delta_1$ (before the break) and $\\delta_2$ (after the break).  $S_{\\rm \\nu,peak}$ is the flux density at peak, and $s$ controls the smoothness of the break.\n",
    "\n",
    "Write a function smooth_broken_power_law() that outputs a smoothed broken power law that also scales based on frequency and spectral index\n",
    "\n",
    "*Hint: in python you can calculate $x^y$ using `x**y`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_broken_power_law(t, nu, S_peak, t_peak, delta_1, delta_2, alpha, log_s, nu0=3.0):\n",
    "\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Markov Chain Monte Carlo\n",
    "\n",
    "We now want to fit a smoothed broken power law to our data. In our paper we do this via a parameter grid-search to minimise the goodness-of-fit parameter $\\chi^2$, i.e., compute $\\chi^2(S_{\\nu,\\rm peak},t_{\\rm peak},\\delta_1,\\delta_2,\\alpha,s)$ for a 6-dimension parameter grid. However, grid searches are slow and innefficient.  It's better to concentrate your effort in the part of the fit where the data \"prefer\" to go.  We can do this using a slightly more complicated statistical technique\n",
    "\n",
    "Here we will perform an Markov Chain Monte Carlo (MCMC) fit using the [`emcee`](http://dfm.io/emcee/current/) package, to determine lightcurve parameters and the spectral index of the source. First you will need to write 3 functions that define your Probability, Prior and Likelihood.\n",
    "\n",
    "We will use a uniform prior with $\\delta_1>0$ (since we require the lightcurve to initially rise), $0<t_{\\rm peak}<300$ (since our data only covers the period up to 200 days) and $s<100$. The parameters will be passed to the function via a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    S_peak, t_peak, delta_1, delta_2, alpha, log_s = theta\n",
    "    \n",
    "    #If the above conditions are met, return 0.0\n",
    "    if #Prior conditions:\n",
    "        return 0.0\n",
    "    \n",
    "    #Otherwise return -infinity\n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write a likelihood function that takes the lightcurve parameters inside the tuple `theta`, along with the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnlike(theta, t, nu, S, S_err):\n",
    "    S_peak, t_peak, delta_1, delta_2, alpha, log_s = theta\n",
    "    \n",
    "    model = smooth_broken_power_law(t, nu, F_peak, t_peak, delta_1, delta_2, alpha, log_s)\n",
    "    \n",
    "    lnlike_val = ######\n",
    "    return lnlike_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a function to calculate the marginal probability using the `lnlike()` and `lnprior()` functions from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(theta, t, nu, S, S_err):\n",
    "    lp = lnprior(theta)\n",
    "\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "\n",
    "    return lp + lnlike(theta, t, nu, S, S_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit the data using the `emcee` package. The function `get_starting_pos()` provided below will set up an array of walker starting positions for given lightcurve parameters. Examine the lightcurve and estimate some reasonable values for these parameters and add them to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_pos(nwalkers, ndim=6):\n",
    "    S_peak = ######\n",
    "    t_peak = ######\n",
    "    delta_1 = ######\n",
    "    delta_2 = ######\n",
    "    alpha = ######\n",
    "    log_s = ######\n",
    "    \n",
    "    pos = [np.asarray([S_peak, t_peak, delta_1, delta_2, alpha, log_s]) + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function called `run_mcmc()` that will load the observed data, take the starting position and then run the emcee Ensemple Sampler. Use a small number of iterations and walkers initially (100/20) to see how long the code takes to run on your laptop. Then increase both parameters to a larger number so that the algorithm takes ~90 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcmc(data, niters=1000, nthreads=1, nwalkers=200, ndim=6):\n",
    "    t = data['delta_t']\n",
    "    nu = data['frequency']\n",
    "    S = data['flux']\n",
    "    S_err = data['rms']\n",
    "    \n",
    "    pos = get_starting_pos(nwalkers, ndim=ndim)\n",
    "    \n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(t, nu, S, S_err), threads=nthreads)\n",
    "    \n",
    "    start = timer()\n",
    "    sampler.run_mcmc(pos, niters)\n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Computation time: %f s\"%(end-start))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to inspect our chain to see if our algorithm has converged to a reasonable solution. We extract the chain from the sampler and then write a function to make a figure showing how each walker moves around the parameter space. The figure has 6 subplots (1 for each dimension), iteration number on the x-axis and parameter value on the y-axis.\n",
    "\n",
    "MCMC algorithms typically use a burn-in phase, where the sampler is moving towards the optimum solution and not yet accurately sampling the parameter space. Add a parameter chain_cut to your function that plots a vertical line at the end of the burn-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_mcmc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-094b1a387627>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_mcmc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mchain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_mcmc' is not defined"
     ]
    }
   ],
   "source": [
    "sampler = run_mcmc(tdata)\n",
    "chain = sampler.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain_plot(chain, chain_cut):\n",
    "    niters = chain.shape[1]\n",
    "    ndim = chain.shape[2]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim,1,sharex=True)\n",
    "    fig.set_size_inches(7, 20)\n",
    "    \n",
    "    param_names = ['$S_{{\\\\nu,\\\\rm peak}, 3\\.{\\\\rm GHz}}$', '$t_{{\\\\rm peak}}$','$\\\\delta_1$','$\\\\delta_2$', '$\\\\alpha$', '$\\\\log_{10}(s)$']\n",
    "\n",
    "    for i, (ax,param_name) in enumerate(zip(axes,param_names)):\n",
    "        #plot the chain for the given parameter\n",
    "        ax.plot(chain[:,:,i].T,linestyle='-',color='k',alpha=0.3)        \n",
    "        \n",
    "        ax.set_ylabel(param_name)\n",
    "        ax.set_xlim(0,niters)\n",
    "        \n",
    "        ax.axvline(chain_cut,c='r',linestyle='--')\n",
    "\n",
    "chain_cut = ######\n",
    "\n",
    "make_chain_plot(chain, chain_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that our algorithm is converging, and we know how long the burn-in takes we can begin to estimate parameters. The function below will make a **corner** plot from the good part of your chain (using the `corner` package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_chain = chain[:, chain_cut:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corner_plot(good_chain, savefile='corner.png'):\n",
    "    param_names = ['$S_{{\\\\nu,\\\\rm peak}, 3\\.{\\\\rm GHz}}$', '$t_{{\\\\rm peak}}$','$\\\\delta_1$','$\\\\delta_2$', '$\\\\alpha$', '$\\\\log_{10}(s)$']\n",
    "    ndim = good_chain.shape[2]\n",
    "    \n",
    "    fig = corner.corner(good_chain.reshape((-1, ndim)), labels=param_names, quantiles=[0.16, 0.5, 0.84], show_titles=True)\n",
    "    plt.savefig(savefile)\n",
    "\n",
    "make_corner_plot(good_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will then extract the median and uncertainty (1 standard deviation) from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(chain):\n",
    "    ndim = chain.shape[2]\n",
    "    \n",
    "    chain = chain.reshape((-1, ndim))\n",
    "    vals = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(chain, [16, 50, 84],axis=0)))\n",
    "    \n",
    "    param_names = ['S_peak', 't_peak', 'delta_1', 'delta_2', 'alpha', 'log_s']\n",
    "    \n",
    "    param_dict = dict(zip(param_names,vals))\n",
    "    \n",
    "    return param_dict\n",
    "    \n",
    "    \n",
    "best_params = get_best_params(good_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function, `calc_chi2()`, that will calculate the $\\chi^2$ for the fit. We will use this later to compare different lightcurve models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chi2(best_params, param_names, model, data, nu0=3.0):\n",
    "    args = []\n",
    "    for param in param_names:\n",
    "        val = \n",
    "        args.append(val)\n",
    "\n",
    "    best_fit = model(data['delta_t'], nu0, *args)\n",
    "    \n",
    "    chi2 = ######\n",
    "    \n",
    "    return chi2\n",
    "\n",
    "param_names = ['F_peak', 't_peak', 'delta_1', 'delta_2', 'alpha', 'log_s']\n",
    "\n",
    "chi2_best = calc_chi2(best_params, param_names, smooth_broken_power_law, tdata)\n",
    "print(chi2_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot our best fit on top of the observational data.\n",
    "\n",
    "Fill in the function `plot_model()` that takes in a function that calculates the model fit (in this case, our `smooth_broken_power_law` function), the best parameters, an array of values to plot the model for and a matplotlib axis to plot it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, params, tvals, ax):\n",
    "    best_fit = ######\n",
    "    \n",
    "    ax.plot(tvals,best_fit,marker='',linestyle='-',c='k',linewidth=1.5,zorder=0)\n",
    "    \n",
    "    return\n",
    "\n",
    "plotting_data = scale_data(tdata, best_params['alpha'][0], np.max(best_params['alpha'][1:]))    \n",
    "    \n",
    "make_plot(plotting_data, scaled=True, model=smooth_broken_power_law, params=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we have fit is motivated by the qualitative shape of the lightcurve. However, there are many physical models that can be calculated, for example using hydrodynamic simulations.\n",
    "\n",
    "The file `jet_cocoon_contribution.txt` contains a two-component lightcurve consisting of the contributions from the jet and cocoon. You can read more about these in some of our papers listed at the end of this exercise.\n",
    "\n",
    "Now write a function to plot the best fitting model for the 3 GHz data by modifying some of the code used in previous parts of this exercise. This function will be called when you pass `plot_models=True` to your `make_plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_physical_models(ax, fname='jet_cocoon_contribution.txt'):\n",
    "    \n",
    "    data = #####\n",
    "    \n",
    "    ax.plot() #Plot the jet contribution\n",
    "    ax.plot() #Plot the cocoon contribution\n",
    "    ax.plot() #Plot the total lightcurve\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(plotting_data, scaled=True, model=smooth_broken_power_law, params=full_args, plot_models=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now determined the final temporal/spectral behavior of the radio data.  What does this tell us?  \n",
    "\n",
    "The fact that the spectral index stayed basically constant over time and that it was the same spectrum from radio to X-ray wavelengths (across ~9 order of magnitude in frequency) tells us that the emission is _optically-thin synchrotron emission_ [Sari et al. (1998)](https://ui.adsabs.harvard.edu/abs/1998ApJ...497L..17S/abstract), which we can use to make other inferences.  We also know that with synchrotron emission you can get a change in the spectrum at high frequencies if the highest-energy electrons have cooled: we see no evidence for a cooling break.  \n",
    "\n",
    "The temporal behavior can be used to constrain the geometry of the emission.  This is what we (and others) did in a series of papers:\n",
    "1. [Hallinan et al. (2017)](http://adsabs.harvard.edu/abs/2017Sci...358.1579H)\n",
    "2. [Alexander et al. (2017)](http://adsabs.harvard.edu/abs/2017ApJ...848L..21A)\n",
    "3. [Mooley et al. (2018a)](http://adsabs.harvard.edu/abs/2018Natur.554..207M)\n",
    "4. [Margutti et al. (2018)](http://adsabs.harvard.edu/abs/2018ApJ...856L..18M)\n",
    "5. [Dobie et al. (2018)](http://adsabs.harvard.edu/abs/2018ApJ...858L..15D)\n",
    "6. [Resmi et al. (2018)](http://adsabs.harvard.edu/abs/2018arXiv180302768R)\n",
    "7. [Alexander et al. (2018)](http://adsabs.harvard.edu/abs/2018ApJ...863L..18A)\n",
    "\n",
    "The latest constraints come from [Mooley et al. (2018b)](https://ui.adsabs.harvard.edu/abs/2018ApJ...868L..11M/abstract), which argues that $\\alpha$ relates to the intrinsic distribution of relativistic electrons as:\n",
    "$$\\alpha = -\\frac{p-1}{2}$$\n",
    "where the electrons have a power-law distribution of energies with index $p$.  Moreover, we can relate the same $p$ to the temporal index in the decay phase in a way that depends on the geomety:\n",
    "1. If the emission is dominated by a jet (collimated, relativistic) we expect $\\delta_2=-p$ in the decay phase\n",
    "2. If it is more spherical it should be:\n",
    "$$\\delta_2 = -3\\frac{p-1}{4}$$\n",
    "(Sari et al. 1998 again).\n",
    "\n",
    "Questions:\n",
    "1. What do you get for $p$ based on your value of $\\alpha$?\n",
    "2. Which scenario, jet or sphere, do you find best fits in the decay phase?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we published the first paper demonstrating evidence of a turnover we only had observations up to 200 days post-merger. We will now determine evidence for a turnover using that subset of data.\n",
    "\n",
    "Create a new table called tdata_200 with the data up to 200 days post-merger and plot the data using your make_plot() function. Using the code from above perform a fit to this subset of the data, calculating the lightcurve parameters and $\\chi^2$ of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-9accf2c02fe9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-9accf2c02fe9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tdata_200 = ######\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tdata_200 = ######\n",
    "\n",
    "sampler = ######\n",
    "\n",
    "good_chain = ######\n",
    "\n",
    "best_params = ######\n",
    "\n",
    "chi2_200 = ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we know that the lightcurve has definitely turned over?\n",
    "\n",
    "We can perform a similar process as above to fit a standard power law to our data and then use an **[F-test](https://en.wikipedia.org/wiki/F-test#Regression_problems)** to determine which model (turnover or no turnover) provides the best fit. We have provided a `power_law()` function that calculates the a power-law fit to the data. Now write a series of functions to perform an MCMC fit using a standard power law; `lnprior_noturnover()`, `lnlike_noturnover()`, `lnprob_noturnover()`, `get_starting_pos_noturnover()`, `run_mcmc_noturnover()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_law(t, nu, F0, delta_1, alpha, nu0=3.0):\n",
    "    return (nu/nu0)**alpha * F0 * t**delta_1\n",
    "\n",
    "def lnprior_noturnover(theta):\n",
    "    F0, delta_1, alpha = theta\n",
    "\n",
    "    if delta_1 > 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    else:\n",
    "        return -np.inf\n",
    "    \n",
    "def lnlike_noturnover(theta, t, nu, S, S_err):\n",
    "    F0, delta_1, alpha = theta\n",
    "\n",
    "    model = ######\n",
    "    inv_sigma2 = ######\n",
    "\n",
    "    lnlike_val = ######\n",
    "\n",
    "    return lnline_val\n",
    "\n",
    "def lnprob_noturnover(theta, t, nu, S, S_err):\n",
    "    lp = lnprior_noturnover(theta)\n",
    "\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "\n",
    "    return lp + lnlike_noturnover(theta, t, nu, S, S_err)\n",
    "\n",
    "def get_starting_pos_noturnover(nwalkers, ndim=6):\n",
    "    F0 = ######\n",
    "    delta_1 = ######\n",
    "    alpha = ######\n",
    "    \n",
    "    pos = [np.asarray([F0, delta_1, alpha]) + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def run_mcmc_noturnover(data, niters=1000, nthreads=1, nwalkers=200, ndim=3):\n",
    "    t = ######\n",
    "    nu = ######\n",
    "    S = ######\n",
    "    S_err = ######\n",
    "    \n",
    "    pos = ######\n",
    "    \n",
    "    sampler = ######\n",
    "    \n",
    "    start = timer()\n",
    "    sampler.run_mcmc(pos, niters)\n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Computation time: %f s\"%(end-start))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the sampler for the standard power law fit and write a function to plot the chains and calculate the length of the burn-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_noturnover = run_mcmc_noturnover(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain_plot_noturnover(chain, chain_cut):\n",
    "    ######\n",
    "\n",
    "chain_noturnover = sampler_noturnover.chain\n",
    "chain_cut_nt = ######\n",
    "\n",
    "make_chain_plot_noturnover(chain_noturnover, chain_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a corner plot for the standard power law fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corner_plot_noturnover(good_chain):\n",
    "    ######\n",
    "\n",
    "good_chain_nt = chain_noturnover[:, chain_cut_nt:, :]\n",
    "\n",
    "make_corner_plot_noturnover(good_chain_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the values of $\\delta_1$ and $\\alpha$ compare to the previous fit. How does the calculated radio spectral index compare to the spectral index determined from fitting data all the way from the radio to X-rays ($\\alpha=-0.585\\pm 0.005$; Margutti et al. 2018, ApJ, 856, L18)?\n",
    "\n",
    "Now write a function to get the best parameters for the standard power law fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params_noturnover(chain):\n",
    "    ######\n",
    "    \n",
    "    \n",
    "    return param_dict\n",
    "    \n",
    "    \n",
    "best_params_nt = get_best_params_noturnover(good_chain_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the best parameters for the standard power law fit we can plot it over our data. Don't forget to scale the data based on the calculated best-fit spectral index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names_nt = ['F0', 'delta_1', 'alpha']\n",
    "args_nt = []\n",
    "for param in param_names_nt:\n",
    "    val = best_params_nt[param][0]\n",
    "    args_nt.append(val)\n",
    "        \n",
    "\n",
    "\n",
    "plotting_data_nt = scale_data(tdata, best_params_nt['alpha'][0],np.max(best_params_nt['alpha'][1:]))    \n",
    "    \n",
    "make_plot(plotting_data_nt,scaled=True,model=power_law,params=args_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the $\\chi^2$ for the standard power law fit. We will then use this, and the previously calculated $\\chi^2$ to perform an F-test and determine which model we prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_nt = calc_chi2(best_params_nt, ['F0', 'delta_1', 'alpha'], power_law, tdata)\n",
    "print(chi2_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [F-test](https://en.wikipedia.org/wiki/F-test) is a generalised test that can be used to compare statistical models. In particular, it is useful when comparing two models where one is a restricted form of the other. Write a function calculate_ftest that calculates the F statistic for our two fits and then calculates the corresponding p-value. Hint: We have already imported the [scipy.stats F-distribution model](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f.html), and we can access the cumulative distribution function using f.cdf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ftest(chi2_t, p_t, chi2_nt, p_nt, n):\n",
    "    F = ######\n",
    "    \n",
    "    pval = f.cdf(F, p_nt, p_t)\n",
    "    \n",
    "    return 1-pval\n",
    "\n",
    "n = ######\n",
    "p_t = ######\n",
    "p_nt = ######\n",
    "\n",
    "calculate_ftest(chi2_best, p_t, chi2_nt, p_nt, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is preferred? With what confidence can we say that we prefer one model over the other?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
